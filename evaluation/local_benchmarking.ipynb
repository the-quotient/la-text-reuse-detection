{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80f3b2-53d2-42e9-a2d1-b74c2a734b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from sklearn.metrics import classification_report\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a8ca51-15c1-4bd5-ae00-035b19ffa6ea",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de6c53f-adf6-496e-a4e3-ee289fee3cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_EVAL_F1 = \"../data/evaluation/OLD_EVAL_F1.csv\"\n",
    "MANUAL_FINDINGS_COMBINED_1000 = \"../data/samples/sample_manual_findings_combined_1000.csv\"\n",
    "\n",
    "# Choose data set \n",
    "data = pd.read_csv(MANUAL_FINDINGS_COMBINED_1000)\n",
    "\n",
    "expected_cols = {'sentence1', 'sentence2', 'label'}\n",
    "if not expected_cols.issubset(data.columns):\n",
    "    raise ValueError(\"CSV file must contain columns: {}\".format(expected_cols))\n",
    "\n",
    "# Relabel: map all labels that are not \"irrelevant\" to 1, and \"irrelevant\" to 0.\n",
    "data['label'] = data['label'].apply(lambda x: 0 if str(x).lower() == \"irrelevant\" else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474e3aa-aee4-468a-b5c0-16c6283c1373",
   "metadata": {},
   "source": [
    "## Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6242c2a0-cbfe-4b70-bfe9-93800daefb08",
   "metadata": {},
   "source": [
    "**Evaluation TODOs** \n",
    "- create datasets for the other tests\n",
    "- implement predefined benchmark tests\n",
    "- configure pipeline for all the tests\n",
    "- log the results to a file (with visualization)\n",
    "- add confusion matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c5a58-0b9c-40d2-845e-caac22d63cad",
   "metadata": {},
   "source": [
    "### Top-k Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623292ba-8289-4dd9-bc2d-c031047a89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVER = \"bowphs/SPhilBERTa\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736021c-bb0a-4e59-8e0c-971543d61be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_embeddings, query_embeddings = compute_embeddings(data, RETRIEVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764318a5-6065-4ecb-82ec-fe49c34b34a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, threshold in product([2, 5, 8], [0.5, 0.6, 0.7, 0.8]):\n",
    "    benchmark_top_k_retrieval_dual(candidate_embeddings, query_embeddings, k=k, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec48a6-f9bd-4570-b772-7a43397ceba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in [0.6, 0.62, 0.64, 0.66, 0.68, 0.7, 0.72, 0.74, 0.76, 0.78, 0.8]:\n",
    "    benchmark_top_k_retrieval_dual(candidate_embeddings, query_embeddings, k=5, threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a412c250-2805-4c2c-93eb-310ae3e4a002",
   "metadata": {},
   "source": [
    "### Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e051101-4360-4895-b888-2a1e986f6772",
   "metadata": {},
   "outputs": [],
   "source": [
    "RERANKER = \"bowphs/LaBERTa\" \n",
    "THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307eac77-774c-46a5-b39d-ac9b2dca4049",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_preds = benchmark_reranker(data, RERANKER, THRESHOLD)\n",
    "report = classification_report(data['label'], reranker_preds)\n",
    "    \n",
    "print(f\"Reranker Baseline Evaluation (threshold = {THRESHOLD}):\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c652c-8192-46cc-ba57-b939db19e830",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed91f543-9674-418e-b77c-f1918326c5b0",
   "metadata": {},
   "source": [
    "### Top-k Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa27dadc-08cb-47ce-97ea-c93ecb8f3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_positive_mapping(data):\n",
    "    \"\"\"\n",
    "    Build a mapping from each unique query (sentence1) to a set of corresponding positive candidates (sentence2)\n",
    "    based on the label (after relabeling: 1 for positive, 0 for \"irrelevant\").\n",
    "    \"\"\"\n",
    "    positive_map = defaultdict(set)\n",
    "    for _, row in data.iterrows():\n",
    "        query = row['sentence1']\n",
    "        candidate = row['sentence2']\n",
    "        label = row['label']  # Assumed: 1 for positive, 0 for irrelevant\n",
    "        if label == 1:\n",
    "            positive_map[query].add(candidate)\n",
    "    return positive_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1828dde-b706-4315-b1ee-d1e995f94356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(data, model_name):\n",
    "    print(\"Loading retrieval model:\", model_name)\n",
    "    retrieval_model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Build mapping of positive candidates per query.\n",
    "    positive_map = build_positive_mapping(data)\n",
    "    \n",
    "    # Obtain all unique queries.\n",
    "    all_queries = data['sentence1'].unique()\n",
    "    positive_queries = [q for q in all_queries if q in positive_map and len(positive_map[q]) > 0]\n",
    "    negative_queries = [q for q in all_queries if q not in positive_map or len(positive_map[q]) == 0]\n",
    "    \n",
    "    # Build candidate pool: use all sentence2 entries.\n",
    "    candidate_sentences = data['sentence2'].tolist()\n",
    "    \n",
    "    print(\"Computing candidate embeddings for the full pool...\")\n",
    "    candidate_embeddings = retrieval_model.encode(candidate_sentences, show_progress_bar=True)\n",
    "    \n",
    "    # Combine queries for evaluation.\n",
    "    evaluation_queries = list(positive_queries) + list(negative_queries)\n",
    "    \n",
    "    print(\"Computing query embeddings for evaluation queries...\")\n",
    "    query_embeddings = retrieval_model.encode(evaluation_queries, show_progress_bar=True)\n",
    "\n",
    "\n",
    "    return candidate_embeddings, query_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e59670-656c-4a61-a0b9-57102f7f2d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_top_k_retrieval_dual(candidate_embeddings, query_embeddings, k, threshold):\n",
    "\n",
    "    # Build mapping of positive candidates per query.\n",
    "    positive_map = build_positive_mapping(data)\n",
    "    \n",
    "    # Obtain all unique queries.\n",
    "    all_queries = data['sentence1'].unique()\n",
    "    positive_queries = [q for q in all_queries if q in positive_map and len(positive_map[q]) > 0]\n",
    "    negative_queries = [q for q in all_queries if q not in positive_map or len(positive_map[q]) == 0]\n",
    "    \n",
    "    # Build candidate pool: use all sentence2 entries.\n",
    "    candidate_sentences = data['sentence2'].tolist()\n",
    "    \n",
    "    # Combine queries for evaluation.\n",
    "    evaluation_queries = list(positive_queries) + list(negative_queries)\n",
    "    \n",
    "    # Initialize metrics counters.\n",
    "    positive_hits = 0      # Count of positive queries with at least one hit.\n",
    "    total_positive = len(positive_queries)\n",
    "    \n",
    "    negative_false_positives = 0  # Count of negative queries with any retrieval.\n",
    "    total_negative = len(negative_queries)\n",
    "    \n",
    "    all_positive_ranks = []       # Best rank among positive queries (np.inf if no hit).\n",
    "    negative_retrieval_counts = []  # Count of retrieved candidates for negative queries.\n",
    "    \n",
    "    for i, query in enumerate(evaluation_queries):\n",
    "        # Use the precomputed embedding.\n",
    "        q_emb = query_embeddings[i]\n",
    "        sims = cosine_similarity([q_emb], candidate_embeddings)[0]\n",
    "        \n",
    "        # Apply threshold filtering.\n",
    "        valid_indices = np.where(sims >= threshold)[0]\n",
    "        if valid_indices.size > 0:\n",
    "            sorted_indices = valid_indices[np.argsort(-sims[valid_indices])]\n",
    "            retrieved_indices = sorted_indices[:k]\n",
    "        else:\n",
    "            retrieved_indices = np.array([])\n",
    "        \n",
    "        # Check whether query is positive or negative.\n",
    "        if query in positive_map and len(positive_map[query]) > 0:\n",
    "            hit = False\n",
    "            best_rank = np.inf\n",
    "            if retrieved_indices.size > 0:\n",
    "                for rank, idx in enumerate(retrieved_indices, start=1):\n",
    "                    if candidate_sentences[idx] in positive_map[query]:\n",
    "                        hit = True\n",
    "                        best_rank = rank\n",
    "                        break\n",
    "            if hit:\n",
    "                positive_hits += 1\n",
    "            all_positive_ranks.append(best_rank)\n",
    "        else:\n",
    "            count_retrieved = len(retrieved_indices)\n",
    "            negative_retrieval_counts.append(count_retrieved)\n",
    "            if count_retrieved > 0:\n",
    "                negative_false_positives += 1\n",
    "\n",
    "    recall_at_k = positive_hits / total_positive if total_positive > 0 else None\n",
    "    false_positive_rate = negative_false_positives / total_negative if total_negative > 0 else None\n",
    "\n",
    "    print(f\"Top-k Retrieval Baseline Evaluation (k = {k}, threshold = {threshold}):\\n\")\n",
    "    print(f\"Positive Queries: {total_positive}\")\n",
    "    print(f\"Hits: {positive_hits}\")\n",
    "    print(f\"Recall@{k}: {recall_at_k:.4f}\\n\")\n",
    "    print(f\"Negative Queries: {total_negative}\")\n",
    "    print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    # Return results along with the computed embeddings for further experimentation.\n",
    "    return {\n",
    "        \"recall_at_k\": recall_at_k,\n",
    "        \"false_positive_rate\": false_positive_rate,\n",
    "        \"positive_ranks\": all_positive_ranks,\n",
    "        \"negative_counts\": negative_retrieval_counts,\n",
    "        \"candidate_embeddings\": candidate_embeddings,\n",
    "        \"query_embeddings\": query_embeddings,\n",
    "        \"evaluation_queries\": evaluation_queries\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338cdb4-41b1-42e4-a8e1-b01300a94c63",
   "metadata": {},
   "source": [
    "### Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac628da-36d6-4680-85aa-5162a27f30c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_reranker(data, model_name, threshold):\n",
    "    cross_encoder = CrossEncoder(model_name)\n",
    "    reranker_preds = []\n",
    "\n",
    "    # Process each sentence pair\n",
    "    for s1, s2 in tqdm(zip(data['sentence1'], data['sentence2']), total=len(data)):\n",
    "        # CrossEncoder expects a list of sentence pairs\n",
    "        # The model returns a continuous score; here we threshold at 0.5 to get a binary label.\n",
    "        score = cross_encoder.predict([(s1, s2)])[0]\n",
    "        pred_label = int(score > threshold)\n",
    "        reranker_preds.append(pred_label)\n",
    "\n",
    "    return reranker_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
