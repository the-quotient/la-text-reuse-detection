{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d5cd2-d565-435e-911c-897100d0750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from benchmarking import BenchmarkRunner\n",
    "MODEL_FOLDER = \"../models/v2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea8dcfb-d033-460d-93aa-d6dfaae63838",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CEP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df15f9-876e-46fa-92dc-19d79976f3e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### EVAL-CEP-S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eff823-b1cf-40ee-915f-b3e267da9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_FOLDER = \"../data/evaluation/eval-tasks-S1/\" \n",
    "RESULT_FOLDER = \"../evaluation/results/CEP/\" \n",
    "\n",
    "TASK_NAME = \"Pa\"\n",
    "LABEL_MAP = \"Pa\"\n",
    "OUTPUT_FILE = \"EVAL-CEP-S1.json\" \n",
    "\n",
    "runner = BenchmarkRunner(MODEL_FOLDER, DATA_FOLDER, RESULT_FOLDER) \n",
    "\n",
    "for model_name in os.listdir(MODEL_FOLDER):\n",
    "    model_path = os.path.join(MODEL_FOLDER, model_name)\n",
    "    if model_name.startswith(\"CEP\") and os.path.isdir(model_path):\n",
    "        print(f\"Benchmarking model: {model_name}\")\n",
    "        runner.benchmark_reranker(\n",
    "            model_name, TASK_NAME, OUTPUT_FILE, LABEL_MAP\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4c3ec-fa8c-445b-9713-ac3f63a1dbfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### EVAL-CEP-S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4182e53-9c9f-435d-92f8-458438d42197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_FOLDER = \"../data/evaluation/eval-tasks-S2/\" \n",
    "RESULT_FOLDER = \"../evaluation/results/CEP/\" \n",
    "\n",
    "TASK_NAME = \"Pa2\"\n",
    "LABEL_MAP = \"Pa\"\n",
    "OUTPUT_FILE = \"EVAL-CEP-S2.json\" \n",
    "\n",
    "runner = BenchmarkRunner(MODEL_FOLDER, DATA_FOLDER, RESULT_FOLDER) \n",
    "\n",
    "for model_name in os.listdir(MODEL_FOLDER):\n",
    "    model_path = os.path.join(MODEL_FOLDER, model_name)\n",
    "    if model_name.startswith(\"CEP\") and os.path.isdir(model_path):\n",
    "        print(f\"Benchmarking model: {model_name}\")\n",
    "        runner.benchmark_reranker(\n",
    "            model_name, TASK_NAME, OUTPUT_FILE, LABEL_MAP\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75884fc4-4830-4e12-991c-c3ae9f9a5842",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### EVAL-CEP-M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d0c5bc-c64c-4a6c-bcc8-25aaa10b94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_FOLDER = \"../data/evaluation/eval-tasks-M1/\" \n",
    "RESULT_FOLDER = \"../evaluation/results/CEP/\" \n",
    "\n",
    "TASK_NAME = \"Pa\"\n",
    "LABEL_MAP = \"Pa\"\n",
    "OUTPUT_FILE = \"EVAL-CEP-M1.json\" \n",
    "\n",
    "runner = BenchmarkRunner(MODEL_FOLDER, DATA_FOLDER, RESULT_FOLDER) \n",
    "\n",
    "for model_name in os.listdir(MODEL_FOLDER):\n",
    "    model_path = os.path.join(MODEL_FOLDER, model_name)\n",
    "    if model_name.startswith(\"CEP\") and os.path.isdir(model_path):\n",
    "        print(f\"Benchmarking model: {model_name}\")\n",
    "        runner.benchmark_reranker(\n",
    "            model_name, TASK_NAME, OUTPUT_FILE, LABEL_MAP\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b1cbf-9cdb-4319-946f-eb43ed7d6404",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### EVAL-CEP-M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cac1b9-806c-4f2f-81e3-b8a255d5da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_FOLDER = \"../data/evaluation/eval-tasks-M2/\" \n",
    "RESULT_FOLDER = \"../evaluation/results/CEP/\" \n",
    "\n",
    "TASK_NAME = \"Pa2\"\n",
    "LABEL_MAP = \"Pa\"\n",
    "OUTPUT_FILE = \"EVAL-CEP-M2.json\" \n",
    "\n",
    "runner = BenchmarkRunner(MODEL_FOLDER, DATA_FOLDER, RESULT_FOLDER) \n",
    "\n",
    "for model_name in os.listdir(MODEL_FOLDER):\n",
    "    model_path = os.path.join(MODEL_FOLDER, model_name)\n",
    "    if model_name.startswith(\"CEP\") and os.path.isdir(model_path):\n",
    "        print(f\"Benchmarking model: {model_name}\")\n",
    "        runner.benchmark_reranker(\n",
    "            model_name, TASK_NAME, OUTPUT_FILE, LABEL_MAP\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842793b1-6339-42aa-8f83-5307deb6e080",
   "metadata": {},
   "source": [
    "## CES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca15e91-e5ae-4a6d-bc6e-19cf320420b9",
   "metadata": {},
   "source": [
    "### EVAL-CES-M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82d2f75-e095-4ac7-ac65-6e7766b1989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../data/evaluation/eval-tasks-S1/\" \n",
    "RESULT_FOLDER = \"../evaluation/results/CES/\" \n",
    "\n",
    "TASK_NAME = \"Pa\"\n",
    "LABEL_MAP = \"Pa\"\n",
    "OUTPUT_FILE = \"EVAL-CES-S1.json\" \n",
    "\n",
    "runner = BenchmarkRunner(MODEL_FOLDER, DATA_FOLDER, RESULT_FOLDER) \n",
    "\n",
    "for model_name in os.listdir(MODEL_FOLDER):\n",
    "    model_path = os.path.join(MODEL_FOLDER, model_name)\n",
    "    if model_name.startswith(\"CES\") and os.path.isdir(model_path):\n",
    "        print(f\"Benchmarking model: {model_name}\")\n",
    "        runner.benchmark_reranker(\n",
    "            model_name, TASK_NAME, OUTPUT_FILE, LABEL_MAP\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb1a043-21a1-4363-9417-8db96eaf68c4",
   "metadata": {},
   "source": [
    "### EVAL-CES-S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7b4bbc-cd48-45de-9385-6e882b15650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_FOLDER = \"../data/evaluation/eval-tasks-M1/\" \n",
    "RESULT_FOLDER = \"../evaluation/results/CES/\" \n",
    "\n",
    "TASK_NAME = \"Pa\"\n",
    "LABEL_MAP = \"Pa\"\n",
    "OUTPUT_FILE = \"EVAL-CES-M1.json\" \n",
    "\n",
    "runner = BenchmarkRunner(MODEL_FOLDER, DATA_FOLDER, RESULT_FOLDER) \n",
    "\n",
    "for model_name in os.listdir(MODEL_FOLDER):\n",
    "    model_path = os.path.join(MODEL_FOLDER, model_name)\n",
    "    if model_name.startswith(\"CES\") and os.path.isdir(model_path):\n",
    "        print(f\"Benchmarking model: {model_name}\")\n",
    "        runner.benchmark_reranker(\n",
    "            model_name, TASK_NAME, OUTPUT_FILE, LABEL_MAP\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dde51c5-9033-4b5d-9504-d3adf9dd34c5",
   "metadata": {},
   "source": [
    "## BE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60910e71-1cad-4f19-8107-72d53dfba350",
   "metadata": {},
   "source": [
    "### BE QS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef232c-6bc3-46ed-b2a3-8cbccf794b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_FOLDER = \"../data/evaluation/eval-tasks-S1/\" \n",
    "RESULT_FOLDER = \"../evaluation/results/BE/\" \n",
    "\n",
    "TASK_NAME = \"Ge\"\n",
    "LABEL_MAP = \"Qu\"\n",
    "OUTPUT_FILE = \"EVAL-BE-QS.json\" \n",
    "\n",
    "runner = BenchmarkRunner(MODEL_FOLDER, DATA_FOLDER, RESULT_FOLDER) \n",
    "\n",
    "for model_name in os.listdir(MODEL_FOLDER):\n",
    "    model_path = os.path.join(MODEL_FOLDER, model_name)\n",
    "    if (model_name.startswith(\"BE\") or model_name.startswith(\"SPhilBERTa\")) and os.path.isdir(model_path):\n",
    "        print(f\"Benchmarking model: {model_name}\")\n",
    "        runner.benchmark_retriever(\n",
    "            model_name, TASK_NAME, OUTPUT_FILE, LABEL_MAP\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077d5d30-57f0-4b97-be5e-d3f7d61fb419",
   "metadata": {},
   "source": [
    "### BE QM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199865e-b993-49b9-9ccb-9f9c2a3f1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_FOLDER = \"../data/evaluation/eval-tasks-M1/\" \n",
    "RESULT_FOLDER = \"../evaluation/results/BE/\" \n",
    "\n",
    "TASK_NAME = \"Ge\"\n",
    "LABEL_MAP = \"Qu\"\n",
    "OUTPUT_FILE = \"EVAL-BE-QM.json\" \n",
    "\n",
    "runner = BenchmarkRunner(MODEL_FOLDER, DATA_FOLDER, RESULT_FOLDER) \n",
    "\n",
    "for model_name in os.listdir(MODEL_FOLDER):\n",
    "    model_path = os.path.join(MODEL_FOLDER, model_name)\n",
    "    if (model_name.startswith(\"BE\") or model_name.startswith(\"SPhilBERTa\")) and os.path.isdir(model_path):\n",
    "        print(f\"Benchmarking model: {model_name}\")\n",
    "        runner.benchmark_retriever(\n",
    "            model_name, TASK_NAME, OUTPUT_FILE, LABEL_MAP\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28518bd2-9929-477b-88f6-3bed935250b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0c6e8-e089-44f7-800c-74f469ff7339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
