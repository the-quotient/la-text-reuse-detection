{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3676f04-8b86-49b0-813e-2f976ab19a6e",
   "metadata": {},
   "source": [
    "# BE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c8474-ffbf-4aa3-a1f2-9d7f9eb2b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_retriever(\"BE/eval_BE_M_Ge.json\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa4521-7839-4cb4-bb9c-416a43d1ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_retriever(\"BE/eval_BE_S_Ge.json\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a42c2-7adf-4e19-bfc3-4d318d78408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_retriever(\"BE/eval-BE-QS.json\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6973d43e-6113-4396-aba9-f247683c19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_retriever(\"BE/eval-BE-QM.json\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c145f6-c998-4720-beca-d6543c03a427",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7375e97-69d7-4715-aec0-829e06823cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reranker(\"CEP/EVAL-CEP-S1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4807f623-65fb-48ac-b12d-0f91435d8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reranker(\"CEP/EVAL-CEP-S2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2fa0ad-a3d4-41c8-a9df-c6bc04811f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reranker(\"CEP/EVAL-CEP-M1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd24ee-b904-464c-915b-c0546f848f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reranker(\"CEP/EVAL-CEP-M2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee630186-bc42-4578-9f2b-091fe5675dd2",
   "metadata": {},
   "source": [
    "# CES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38793c20-cb8b-4291-b849-35ac70e9dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reranker(\"CES/EVAL-CES-S1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481f424-b839-4fc8-95ea-70265e009968",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reranker(\"CES/EVAL-CES-M1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700bcbe6-e508-400e-ac98-c7443a6184a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Plotting Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068e8099-0231-46fe-969d-34a0dccb1823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43260a11-3c47-4df4-a129-48ebf138c7e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_retriever(file_path, k):\n",
    "\n",
    "    path = Path(f\"results/{file_path}\")\n",
    "    with open(path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    records = []\n",
    "    for entry in data:\n",
    "        model = entry[\"model\"]\n",
    "        for result in entry[\"results\"]:\n",
    "            if result[\"k\"] == k:\n",
    "                threshold = result[\"threshold\"]\n",
    "                precision = result[\"precision@k\"]\n",
    "                recall = result[\"recall@k\"]\n",
    "                f1 = result[\"f1@k\"]\n",
    "                fpr = result[\"false_positive_rate\"]\n",
    "                records.append({\n",
    "                    \"model\": model,\n",
    "                    \"threshold\": threshold,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"f1\": f1,\n",
    "                    \"false_positive_rate\": fpr\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    avg_metrics = df.groupby([\"model\", \"threshold\"]).mean().reset_index()\n",
    "\n",
    "\n",
    "    # Plotting recall vs false positive rate\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    # Plot by model\n",
    "    for model_name, group in avg_metrics.groupby(\"model\"):\n",
    "        plt.plot(group[\"false_positive_rate\"], group[\"recall\"], marker='o', label=model_name)\n",
    "\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.title(f\"Recall vs. False Positive Rate (k={k})\")\n",
    "    plt.legend(title=\"Model\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"pdf/{file_path}_REvFPR.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting metrics vs threshold\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(6, 9), sharex=True)\n",
    "\n",
    "    # Precision vs Threshold\n",
    "    for model_name, group in avg_metrics.groupby(\"model\"):\n",
    "        axes[0].plot(group[\"threshold\"], group[\"precision\"], marker='o', label=model_name)\n",
    "        axes[0].set_ylabel(\"Precision\")\n",
    "        axes[0].set_title(\"Precision vs. Threshold\")\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "\n",
    "    # Recall vs Threshold\n",
    "    for model_name, group in avg_metrics.groupby(\"model\"):\n",
    "        axes[1].plot(group[\"threshold\"], group[\"recall\"], marker='o', label=model_name)\n",
    "        axes[1].set_ylabel(\"Recall\")\n",
    "        axes[1].set_title(\"Recall vs. Threshold\")\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"pdf/{file_path}_PRaRE.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_reranker(file_path):\n",
    "    path = Path(f\"results/{file_path}\")\n",
    "    with open(path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    records = []\n",
    "    for entry in data:\n",
    "        model = entry[\"model\"]\n",
    "        threshold = entry[\"threshold\"]\n",
    "        report = entry[\"classification_report\"]\n",
    "\n",
    "        precision_0 = report[\"0\"][\"precision\"]\n",
    "        recall_0 = report[\"0\"][\"recall\"]\n",
    "        precision_1 = report[\"1\"][\"precision\"]\n",
    "        recall_1 = report[\"1\"][\"recall\"]\n",
    "        support_1 = report[\"1\"][\"support\"]\n",
    "\n",
    "        # Compute false positives for class 1\n",
    "        tp_1 = recall_1 * support_1\n",
    "        fp_1 = tp_1 * (1 / precision_1 - 1) if precision_1 else 0\n",
    "\n",
    "        # Compute F1 scores\n",
    "        f1_0 = (2 * precision_0 * recall_0) / (precision_0 + recall_0) \\\n",
    "               if (precision_0 + recall_0) else 0\n",
    "        f1_1 = (2 * precision_1 * recall_1) / (precision_1 + recall_1) \\\n",
    "               if (precision_1 + recall_1) else 0\n",
    "\n",
    "        records.append({\n",
    "            \"model\": model,\n",
    "            \"threshold\": threshold,\n",
    "            \"precision_0\": precision_0,\n",
    "            \"recall_0\": recall_0,\n",
    "            \"precision_1\": precision_1,\n",
    "            \"recall_1\": recall_1,\n",
    "            \"false_positives_1\": fp_1,\n",
    "            \"f1_0\": f1_0,\n",
    "            \"f1_1\": f1_1\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df[\"avg_precision\"] = (df[\"precision_0\"] + df[\"precision_1\"]) / 2\n",
    "    df[\"avg_recall\"] = (df[\"recall_0\"] + df[\"recall_1\"]) / 2\n",
    "\n",
    "    grouped = df.groupby([\"model\", \"threshold\"]).mean().reset_index()\n",
    "    thresholds = sorted(grouped[\"threshold\"].unique())\n",
    "    models = grouped[\"model\"].unique()\n",
    "    x = range(len(models))\n",
    "\n",
    "    # --- Third Figure: F1 Scores ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for threshold in thresholds:\n",
    "        sub_df = grouped[grouped[\"threshold\"] == threshold]\n",
    "        plt.plot(x, sub_df[\"f1_0\"], marker='o', linestyle='-',\n",
    "                 label=f'F1 Class 0 (th={threshold})')\n",
    "        plt.plot(x, sub_df[\"f1_1\"], marker='s', linestyle='--',\n",
    "                 label=f'F1 Class 1 (th={threshold})')\n",
    "\n",
    "    plt.xticks(x, models, rotation=45)\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(\"F1 Scores by Class and Threshold\")\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1.01, 1.0))\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    plt.savefig(f\"pdf/{file_path}_F1.pdf\",\n",
    "                dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aebf1fd-dd7f-4003-9095-5a31bb5ae202",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # --- First Figure: Precision and Recall Plots ---\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(10, 11), sharex=True)\n",
    "    metric_keys = [\"precision_0\", \"recall_0\", \"precision_1\", \"recall_1\"]\n",
    "    titles = [\"Precision (Class 0)\", \"Recall (Class 0)\",\n",
    "              \"Precision (Class 1)\", \"Recall (Class 1)\"]\n",
    "\n",
    "    for ax, metric, title in zip(axes, metric_keys, titles):\n",
    "        for threshold in thresholds:\n",
    "            sub_df = grouped[grouped[\"threshold\"] == threshold]\n",
    "            ax.plot(x, sub_df[metric], marker='o', label=f'th={threshold}')\n",
    "        ax.set_ylabel(\"Score\")\n",
    "        ax.set_title(title)\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(1.01, 1.0))\n",
    "\n",
    "    axes[-1].set_xticks(x)\n",
    "    axes[-1].set_xticklabels(models, rotation=45)\n",
    "    axes[-1].set_xlabel(\"Model\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    plt.savefig(f\"pdf/{file_path}_PRaRE.pdf\",\n",
    "                dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- Second Figure: Recall vs. False Positives (Class 1) ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for model in models:\n",
    "        sub_df = grouped[grouped[\"model\"] == model]\n",
    "        plt.plot(sub_df[\"false_positives_1\"], sub_df[\"recall_1\"],\n",
    "                 marker='x', linestyle='--', label=f'{model}')\n",
    "\n",
    "    plt.xlabel(\"False Positives (Class 1)\")\n",
    "    plt.ylabel(\"Recall (Class 1)\")\n",
    "    plt.title(\"Recall vs. False Positives (Class 1)\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"pdf/{file_path}_REvFPR.pdf\",\n",
    "                dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a1b32-4a1f-4f80-8b65-464c70d1f4fe",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98999f6-14a8-4191-9fc0-969ccb11b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def conf_mat(name, file_path, output_dir=\"confusion_matrices\"):\n",
    "    # Load the new JSON data\n",
    "    file_path = Path(file_path)\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extract unique threshold_p values\n",
    "    threshold_values = sorted(set(entry[\"reranker_p_threshold\"] for entry in data))\n",
    "    \n",
    "    # Set the label set\n",
    "    label_set = [\"quote\", \"fuzzy_quote\", \"paraphrase\", \"similar_sentence\", \"irrelevant\"]\n",
    "    label_to_index = {label: i for i, label in enumerate(label_set)}\n",
    "    n_labels = len(label_set)\n",
    "\n",
    "    # Create output directory\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Dictionary to store confusion matrices by threshold\n",
    "    confusion_matrices_by_threshold = {}\n",
    "    \n",
    "    # Aggregate confusion matrices for each threshold\n",
    "    for threshold in threshold_values:\n",
    "        aggregated_matrix = np.zeros((n_labels, n_labels), dtype=int)\n",
    "        for entry in data:\n",
    "            if entry[\"reranker_p_threshold\"] == threshold:\n",
    "                aggregated_matrix += np.array(entry[\"confusion_matrix\"][\"matrix\"])\n",
    "        confusion_matrices_by_threshold[threshold] = aggregated_matrix\n",
    "    \n",
    "    # Plotting all confusion matrices\n",
    "    for threshold, matrix in confusion_matrices_by_threshold.items():\n",
    "        normalized_matrix = matrix.astype('float') / matrix.sum(axis=1, keepdims=True)\n",
    "    \n",
    "        fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "        ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=label_set).plot(\n",
    "            ax=ax1, cmap=\"Blues\", xticks_rotation=45\n",
    "        )\n",
    "        ax1.set_title(f\"Confusion Matrix\")\n",
    "        fig1.tight_layout()\n",
    "        fig1.savefig(output_path / f\"confusion_matrix_{name}_{threshold}.pdf\")\n",
    "\n",
    "        fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "        ConfusionMatrixDisplay(confusion_matrix=normalized_matrix, display_labels=label_set).plot(\n",
    "            ax=ax2, cmap=\"Blues\", xticks_rotation=45\n",
    "        )\n",
    "        ax2.set_title(f\"Normalized Confusion\")\n",
    "        fig2.tight_layout()\n",
    "        fig2.savefig(output_path / f\"confusion_matrix_{name}_{threshold}_normalized.pdf\")\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0ac33a-4917-438b-8739-5e0b45fce53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat(\"PL-S\", \"results/PL/EVAL-PL-S.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a6476-4f50-4af3-9746-68b16fd5c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat(\"results/PL/EVAL-PL-M.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d31dd-fa91-491a-bd6b-d51f831c1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat(\"results/PL/EVAL2-PL-S.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f3fbf-c3f3-41d9-be4d-7ab8c5d7e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat(\"PL-M\", \"results/PL/EVAL2-PL-M.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e80fb-938f-4f5e-bdad-8440adfad718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
