{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f11336-7291-44c3-aa73-6a112c16926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143475f6-277a-4124-9299-18714619d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampling import Sampler \n",
    "\n",
    "sampler = Sampler(\n",
    "    model_path=\"../../models/SPhilBERTa\",\n",
    "    paraphrases_path=\"../../data/llm-completed-pairs/test_paraphrases.json\",\n",
    "    similar_sentences_path=\"../../data/llm-completed-pairs/test_similar_sentences.json\",\n",
    "    corpus_path=\"../../data/corpus/corpus/corpus.jsonl\"\n",
    ")\n",
    "\n",
    "negatives = sampler.sample_negatives_irrelevant()\n",
    "sampler.save(negatives, label=\"irrelevant\", output_file=\"negatives.json\")\n",
    "for pair in negatives:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf96a024-3c3d-4c82-92da-033b7933a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_quotes(corpus_file, output_file, sample_size):\n",
    "    data = []\n",
    "    with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    record = json.loads(line)\n",
    "                    data.append(record[\"sentence\"])\n",
    "\n",
    "    sample = random.sample(data, sample_size)\n",
    "\n",
    "    output = []\n",
    "    for sentence in sample:\n",
    "        output.append({\n",
    "            \"sentence1\": sentence,\n",
    "            \"sentence2\": sentence,\n",
    "            \"label\": \"quote\"\n",
    "        })\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aebca4cd-dd63-4432-b626-f17f162241a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_quotes(\"../../data/corpus/corpus/corpus.jsonl\", \"../../data/training/retriever/quotes.json\", 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43536cee-42e1-4aa4-b9db-195ad8fb9254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    data = []\n",
    "\n",
    "    if ext == '.jsonl':\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    record = json.loads(line)\n",
    "                    data.append(record[\"sentence\"])\n",
    "    elif ext == '.json':\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            json_data = json.load(f)\n",
    "            for record in json_data:\n",
    "                sent1 = record.get(\"sentence1\")\n",
    "                sent2 = record.get(\"sentence2\")\n",
    "                if sent1 is not None and sent2 is not None:\n",
    "                    data.append((sent1, sent2))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file extension. Only .json and .jsonl files are supported.\")\n",
    "\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "def save(pairs, output_file):\n",
    "    shuffled = [\n",
    "        tup if random.random() < 0.5 else tup[::-1]\n",
    "        for tup in pairs\n",
    "    ]\n",
    "    output = []\n",
    "    for sentence1, sentence2 in shuffled:\n",
    "        output.append({\n",
    "            \"sentence1\": sentence1,\n",
    "            \"sentence2\": sentence2,\n",
    "            \"label\": \"irrelevant\"\n",
    "        })\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=4)\n",
    "\n",
    "def sample_irrelevant_for_pairs(corpus_file, pairs_file):\n",
    "\n",
    "    corpus = load(corpus_file)\n",
    "    corpus_set = set(corpus)\n",
    "    pairs = load(pairs_file)\n",
    "    negatives = []\n",
    "    \n",
    "    for pair in pairs:\n",
    "        s1, s2 = pair\n",
    "        if s1 in corpus_set:\n",
    "            original = s1\n",
    "        elif s2 in corpus_set:\n",
    "            original = s2\n",
    "        else:\n",
    "            continue\n",
    "        negatives.append((original, random.choice(corpus)))\n",
    "\n",
    "    return negatives\n",
    "\n",
    "def sample_irrelevant(corpus_file, number):\n",
    "\n",
    "    corpus = load(corpus_file)\n",
    "    pairs = [] \n",
    "\n",
    "    for i in range(0, number):\n",
    "        s1 = random.choice(corpus)\n",
    "        s2 = random.choice(corpus)\n",
    "        if s1 != s2:\n",
    "            pairs.append((s1,s2)) \n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d8b9e8c-68a1-4cd3-9468-91cff13df631",
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = sample_irrelevant(\"../../data/corpus/corpus/corpus.jsonl\", 1000)\n",
    "save(negatives, \"../../data/evaluation/I1000.json\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cce3302c-c74e-4558-9220-6280d5acba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = sample_irrelevant(\"../../data/corpus/corpus/corpus.jsonl\", \"../../data/training/v2/retriever/paraphrases.json\")\n",
    "save(negatives, \"../../data/training/v2/retriever/irrelevant_for_paraphrases.json\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43018467-ed20-4294-a2a9-2080ee85a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = sample_irrelevant(\"../../data/corpus/corpus/corpus.jsonl\", \"../../data/training/v2/retriever/similar_sentences.json\")\n",
    "save(negatives, \"../../data/training/v2/retriever/irrelevant_for_similar_sentences.json\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3db444d-33c3-4662-89f5-c4a9a8439e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplets(file1_path, file2_path, output_path):\n",
    "    \n",
    "    def collect_pairs(data):\n",
    "        pairs = {}\n",
    "        for item in data:\n",
    "            s1, s2, label = item[\"sentence1\"], item[\"sentence2\"], item[\"label\"]\n",
    "            for anchor, other in [(s1, s2), (s2, s1)]:\n",
    "                pairs.setdefault(anchor, []).append((other, label))\n",
    "        return pairs\n",
    "\n",
    "    # Load and process files\n",
    "    with open(file1_path, 'r', encoding='utf-8') as f:\n",
    "        data1 = json.load(f)\n",
    "    with open(file2_path, 'r', encoding='utf-8') as f:\n",
    "        data2 = json.load(f)\n",
    "\n",
    "    map1 = collect_pairs(data1)\n",
    "    map2 = collect_pairs(data2)\n",
    "\n",
    "    triplets = []\n",
    "    for anchor in map1:\n",
    "        if anchor in map2:\n",
    "            for other1, label1 in map1[anchor]:\n",
    "                for other2, label2 in map2[anchor]:\n",
    "                    if label1 != label2:\n",
    "                        if label1 == \"irrelevant\":\n",
    "                            negative = other1\n",
    "                            positive = other2\n",
    "                        else:\n",
    "                            negative = other2\n",
    "                            positive = other1\n",
    "                        triplets.append({\n",
    "                            \"anchor\": anchor,\n",
    "                            \"positive\": positive,\n",
    "                            \"negative\": negative\n",
    "                        })\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(triplets, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5562fda5-4dbb-4982-bfb7-d78b9a4b4548",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_triplets(\n",
    "    \"../../data/training/v2/fuzzy_quotes.json\", \n",
    "    \"../../data/training/v2/irrelevant_for_fuzzy_quotes.json\", \n",
    "    \"../../data/training/v2/triplets_fuzzy_quotes.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51fdde5b-16fa-49c1-94ef-202f7cdbc6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_triplets(\n",
    "    \"../../data/training/v2/paraphrases.json\", \n",
    "    \"../../data/training/v2/irrelevant_for_paraphrases.json\", \n",
    "    \"../../data/training/v2/triplets_paraphrases.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac4a8c8c-1e6a-49d6-9b0c-34fd4f404d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_triplets(\n",
    "    \"../../data/training/v2/similar_sentences.json\", \n",
    "    \"../../data/training/v2/irrelevant_for_similar_sentences.json\", \n",
    "    \"../../data/training/v2/triplets_similar_sentences.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87fa2d86-4fe2-4e3d-a27c-2bfe1c4a1f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 371745\n",
      "Min length: 11\n",
      "Max length: 763\n",
      "Mean length: 29.733177312405008\n",
      "Median length: 25.0\n",
      "90th percentile: 49.0\n",
      "95th percentile: 60.0\n",
      "99th percentile: 87.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the tokenizer (CamemBERT is compatible with SPhilBERTa)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bowphs/LaBERTa\")\n",
    "\n",
    "# Path to your JSONL file\n",
    "file_path = \"../../data/corpus/corpus/corpus.jsonl\"\n",
    "\n",
    "# Load sentences\n",
    "sentences = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        sentences.append(data[\"sentence\"])\n",
    "\n",
    "# Tokenize and compute lengths\n",
    "lengths = [len(tokenizer.encode(s, truncation=False)) for s in sentences]\n",
    "\n",
    "# Show stats\n",
    "print(\"Total sentences:\", len(lengths))\n",
    "print(\"Min length:\", min(lengths))\n",
    "print(\"Max length:\", max(lengths))\n",
    "print(\"Mean length:\", np.mean(lengths))\n",
    "print(\"Median length:\", np.median(lengths))\n",
    "print(\"90th percentile:\", np.percentile(lengths, 90))\n",
    "print(\"95th percentile:\", np.percentile(lengths, 95))\n",
    "print(\"99th percentile:\", np.percentile(lengths, 99))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8488b67-f8d8-4038-9a66-95caeb7e8583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
