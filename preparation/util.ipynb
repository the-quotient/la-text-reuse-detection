{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f88dc691-3b2a-46a7-a9aa-1cd3e360da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "def split_json_by_label(input_file, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Group entries by their label\n",
    "    grouped = defaultdict(list)\n",
    "    for entry in data:\n",
    "        label = entry.get(\"label\", \"unlabeled\")\n",
    "        grouped[label].append(entry)\n",
    "\n",
    "    # Write each group into a separate file\n",
    "    for label, entries in grouped.items():\n",
    "        output_path = os.path.join(output_dir, f\"{label}.json\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(entries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Split complete. Files saved in: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc905093-edf5-4e9c-9a58-bf17675f893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_json_by_label(\"../data/evaluation/GMF_1.json\", \"../data/evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1962567d-7113-429a-a527-eb0aa3a4253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def split_nested_json_by_top_level_key(input_file, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # For each top-level key, treat it as a label\n",
    "    for label, pairs in data.items():\n",
    "        entries = []\n",
    "        for pair in pairs:\n",
    "            entry = {\n",
    "                \"sentence1\": pair.get(\"query\", \"\"),\n",
    "                \"sentence2\": pair.get(\"candidate\", \"\"),\n",
    "                \"label\": label\n",
    "            }\n",
    "            entries.append(entry)\n",
    "\n",
    "        # Write to a file named after the label\n",
    "        output_path = os.path.join(output_dir, f\"{label}.json\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(entries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Files written to '{output_dir}' for labels: {', '.join(data.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa7b3ca-a199-4482-a552-4f25cfb36977",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_nested_json_by_top_level_key(\"../data/evaluation/Comp_Wild_Zwingli.json\", \"../data/evaluation/MF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa0149-96eb-47c8-80d8-96c3240a4dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "LABEL = {\n",
    "    'Qu': 'quote',\n",
    "    'Fu': 'fuzzy_quote',\n",
    "    'Pa': 'paraphrase',\n",
    "    'Si': 'similar_sentence'\n",
    "}\n",
    "\n",
    "GENERAL_DISTRIBUTION = {\n",
    "    'Qu': 1,\n",
    "    'Fu': 3,\n",
    "    'Pa': 2,\n",
    "    'Si': 4\n",
    "}\n",
    "\n",
    "\n",
    "def load_labeled_pairs(file_path, label):\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return [\n",
    "        {\n",
    "            'sentence1': pair['sentence1'],\n",
    "            'sentence2': pair['sentence2'],\n",
    "            'label': LABEL[label]\n",
    "        }\n",
    "        for pair in data\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_corpus_sentences(corpus_path):\n",
    "    with open(corpus_path, encoding='utf-8') as f:\n",
    "        return [json.loads(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "\n",
    "def sample_irrelevant_pairs(lines, num_samples):\n",
    "    pairs = []\n",
    "    seen = set()\n",
    "    n = len(lines)\n",
    "    while len(pairs) < num_samples:\n",
    "        i, j = random.sample(range(n), 2)\n",
    "        s1, s2 = lines[i]['sentence'], lines[j]['sentence']\n",
    "        if s1 != s2 and (i, j) not in seen:\n",
    "            seen.add((i, j))\n",
    "            pairs.append({\n",
    "                'sentence1': s1,\n",
    "                'sentence2': s2,\n",
    "                'label': 'irrelevant'\n",
    "            })\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def save_eval_file(pairs, output_root, prefix, folder_label, index):\n",
    "    output_root.mkdir(parents=True, exist_ok=True)\n",
    "    filename = f\"{prefix}{folder_label}_{index:02d}.json\"\n",
    "    out_path = output_root / filename\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(pairs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def generate_category_files(folder_path, corpus_lines, output_path):\n",
    "    folder_label = folder_path.name\n",
    "    for prefix, full_label in LABEL.items():\n",
    "        labeled_path = folder_path / f\"{LABEL[prefix]}.json\"\n",
    "        labeled_pairs = load_labeled_pairs(labeled_path, prefix)\n",
    "\n",
    "        num_chunks = len(labeled_pairs) // 10\n",
    "        for i in range(num_chunks):\n",
    "            chunk_labeled = labeled_pairs[i * 10:(i + 1) * 10]\n",
    "            sampled_irrelevant = sample_irrelevant_pairs(corpus_lines, 90)\n",
    "            full = chunk_labeled + sampled_irrelevant\n",
    "            random.shuffle(full)\n",
    "            save_eval_file(full, output_path, prefix, folder_label, i)\n",
    "\n",
    "\n",
    "def generate_general_files(folder_path, corpus_lines, output_path):\n",
    "    folder_label = folder_path.name\n",
    "    all_labeled = {\n",
    "        key: load_labeled_pairs(folder_path / f\"{fname}.json\", key)\n",
    "        for key, fname in LABEL.items()\n",
    "    }\n",
    "\n",
    "    num_sets = min(\n",
    "        len(all_labeled['Qu']) // 1,\n",
    "        len(all_labeled['Fu']) // 3,\n",
    "        len(all_labeled['Pa']) // 2,\n",
    "        len(all_labeled['Si']) // 4\n",
    "    )\n",
    "\n",
    "    for i in range(num_sets):\n",
    "        block = []\n",
    "        for key, count in GENERAL_DISTRIBUTION.items():\n",
    "            start = i * count\n",
    "            block.extend(all_labeled[key][start:start + count])\n",
    "        sampled_irrelevant = sample_irrelevant_pairs(corpus_lines, 990)\n",
    "        full = block + sampled_irrelevant\n",
    "        random.shuffle(full)\n",
    "        save_eval_file(full, output_path, 'Ge', folder_label, i)\n",
    "\n",
    "def generate_custom_files(folder_path, corpus_lines, output_path):\n",
    "    folder_label = folder_path.name\n",
    "    all_labeled = {\n",
    "        key: load_labeled_pairs(folder_path / f\"{fname}.json\", key)\n",
    "        for key, fname in LABEL.items()\n",
    "    }\n",
    "\n",
    "    def make_file(label, counts, prefix):\n",
    "        min_sets = min(\n",
    "            len(all_labeled[k]) // v for k, v in counts.items() if k != 'Ir'\n",
    "        )\n",
    "        for i in range(min_sets):\n",
    "            block = []\n",
    "            for k, v in counts.items():\n",
    "                if k == 'Ir':\n",
    "                    continue\n",
    "                start = i * v\n",
    "                block.extend(all_labeled[k][start:start + v])\n",
    "            num_irrelevant = counts.get('Ir', 0)\n",
    "            block.extend(sample_irrelevant_pairs(corpus_lines, num_irrelevant))\n",
    "            random.shuffle(block)\n",
    "            save_eval_file(block, output_path, prefix, folder_label, i)\n",
    "\n",
    "    # Define custom configurations\n",
    "    fu2_counts = {'Fu': 10, 'Pa': 10, 'Si': 10, 'Ir': 70}\n",
    "    pa2_counts = {'Pa': 10, 'Si': 10, 'Ir': 80}\n",
    "\n",
    "    make_file('Fu2', fu2_counts, 'Fu2')\n",
    "    make_file('Pa2', pa2_counts, 'Pa2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ddd4c-3eb7-4e43-932c-689467b61440",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lines = load_corpus_sentences(Path(\"../data/corpus/corpus/corpus.jsonl\"))\n",
    "generate_custom_files(Path(\"../data/evaluation/eval-task-sources/M\"), corpus_lines, Path(\"../data/evaluation/eval-tasks-M2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d458bc5-e2c9-4f86-aa5c-adf75c97dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lines = load_corpus_sentences(Path(\"../data/corpus/corpus/corpus.jsonl\"))\n",
    "generate_custom_files(Path(\"../data/evaluation/eval-task-sources/S\"), corpus_lines, Path(\"../data/evaluation/eval-tasks-S2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc7049a-c5c2-4f86-9d40-f1833571739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lines = load_corpus_sentences(Path(\"../data/corpus/corpus/corpus.jsonl\"))\n",
    "\n",
    "generate_category_files(Path(\"../data/evaluation/eval-task-sources/S\"), corpus_lines, Path(\"../data/evaluation/eval-tasks-S1\"))\n",
    "#generate_general_files(Path(\"data/evaluation/eval-task-sources/S\"), corpus_lines, Path(\"data/evaluation/eval-tasks\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc5c2e-b432-4ad2-bd0a-80dbcba5ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lines = load_corpus_sentences(Path(\"../data/corpus/corpus/corpus.jsonl\"))\n",
    "\n",
    "generate_category_files(Path(\"../data/evaluation/eval-task-sources/M\"), corpus_lines, Path(\"../data/evaluation/eval-tasks-M1\"))\n",
    "#generate_general_files(Path(\"../data/evaluation/eval-task-sources/M\"), corpus_lines, Path(\"../data/evaluation/eval-tasks-M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dff997-3b32-4b13-9360-cf21c968073c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31335c8d-c42e-4a86-9bb3-3cf836ee20f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def analyze_jsonl_folder(\n",
    "    folder_path: str,\n",
    "    text_key: str = \"sentence\",\n",
    "    model: str = \"la_core_web_lg\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Analyzes .jsonl files in a folder for sentence and token counts,\n",
    "    printing results at the end in JSON format.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(model, disable=[\"parser\", \"ner\"])\n",
    "    total_sentences = 0\n",
    "    total_tokens = 0\n",
    "    file_stats = []\n",
    "\n",
    "    jsonl_files = [\n",
    "        f for f in os.listdir(folder_path) if f.endswith(\".jsonl\")\n",
    "    ]\n",
    "\n",
    "    for filename in tqdm(jsonl_files, desc=\"Files\", unit=\"file\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        sentence_count = 0\n",
    "        token_count = 0\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                line_count = sum(1 for _ in f)\n",
    "        except Exception as e:\n",
    "            continue \n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in tqdm(f, total=line_count,\n",
    "                             desc=filename, unit=\"line\", leave=False):\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    text = data.get(text_key, \"\")\n",
    "                    doc = nlp(text)\n",
    "                    tokens = [t for t in doc if not t.is_space]\n",
    "                    sentence_count += 1\n",
    "                    token_count += len(tokens)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "        file_stats.append({\n",
    "            \"file\": filename,\n",
    "            \"sentences\": sentence_count,\n",
    "            \"tokens\": token_count\n",
    "        })\n",
    "\n",
    "        total_sentences += sentence_count\n",
    "        total_tokens += token_count\n",
    "\n",
    "    result = {\n",
    "        \"files\": file_stats,\n",
    "        \"total\": {\n",
    "            \"sentences\": total_sentences,\n",
    "            \"tokens\": total_tokens\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd42d6f-9dc4-420e-8324-92efbbb042ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_jsonl_folder(\"../data/corpus/documents/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb7e0a-c650-4f21-a83c-83fc2ac4c2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
