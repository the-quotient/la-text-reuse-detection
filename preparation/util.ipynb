{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f88dc691-3b2a-46a7-a9aa-1cd3e360da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "def split_json_by_label(input_file, output_dir):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Read the original JSON file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Group entries by their label\n",
    "    grouped = defaultdict(list)\n",
    "    for entry in data:\n",
    "        label = entry.get(\"label\", \"unlabeled\")\n",
    "        grouped[label].append(entry)\n",
    "\n",
    "    # Write each group into a separate file\n",
    "    for label, entries in grouped.items():\n",
    "        output_path = os.path.join(output_dir, f\"{label}.json\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(entries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Split complete. Files saved in: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc905093-edf5-4e9c-9a58-bf17675f893d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split complete. Files saved in: data/samples\n"
     ]
    }
   ],
   "source": [
    "split_json_by_label(\"data/samples/GMF_1.json\", \"data/samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1962567d-7113-429a-a527-eb0aa3a4253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def split_nested_json_by_top_level_key(input_file, output_dir):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load input JSON\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # For each top-level key, treat it as a label\n",
    "    for label, pairs in data.items():\n",
    "        entries = []\n",
    "        for pair in pairs:\n",
    "            entry = {\n",
    "                \"sentence1\": pair.get(\"query\", \"\"),\n",
    "                \"sentence2\": pair.get(\"candidate\", \"\"),\n",
    "                \"label\": label\n",
    "            }\n",
    "            entries.append(entry)\n",
    "\n",
    "        # Write to a file named after the label\n",
    "        output_path = os.path.join(output_dir, f\"{label}.json\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(entries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Files written to '{output_dir}' for labels: {', '.join(data.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fa7b3ca-a199-4482-a552-4f25cfb36977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files written to 'data/samples/MF' for labels: quote, fuzzy_quote, paraphrase, similar_sentence, irrelevant\n"
     ]
    }
   ],
   "source": [
    "split_nested_json_by_top_level_key(\"data/samples/Comp_Wild_Zwingli.json\", \"data/samples/MF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40fa0149-96eb-47c8-80d8-96c3240a4dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "LABEL = {\n",
    "    'Qu': 'quote',\n",
    "    'Fu': 'fuzzy_quote',\n",
    "    'Pa': 'paraphrase',\n",
    "    'Si': 'similar_sentence'\n",
    "}\n",
    "\n",
    "GENERAL_DISTRIBUTION = {\n",
    "    'Qu': 1,\n",
    "    'Fu': 3,\n",
    "    'Pa': 2,\n",
    "    'Si': 4\n",
    "}\n",
    "\n",
    "\n",
    "def load_labeled_pairs(file_path, label):\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return [\n",
    "        {\n",
    "            'sentence1': pair['sentence1'],\n",
    "            'sentence2': pair['sentence2'],\n",
    "            'label': LABEL[label]\n",
    "        }\n",
    "        for pair in data\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_corpus_sentences(corpus_path):\n",
    "    with open(corpus_path, encoding='utf-8') as f:\n",
    "        return [json.loads(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "\n",
    "def sample_irrelevant_pairs(lines, num_samples):\n",
    "    pairs = []\n",
    "    seen = set()\n",
    "    n = len(lines)\n",
    "    while len(pairs) < num_samples:\n",
    "        i, j = random.sample(range(n), 2)\n",
    "        s1, s2 = lines[i]['sentence'], lines[j]['sentence']\n",
    "        if s1 != s2 and (i, j) not in seen:\n",
    "            seen.add((i, j))\n",
    "            pairs.append({\n",
    "                'sentence1': s1,\n",
    "                'sentence2': s2,\n",
    "                'label': 'irrelevant'\n",
    "            })\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def save_eval_file(pairs, output_root, prefix, folder_label, index):\n",
    "    output_root.mkdir(parents=True, exist_ok=True)\n",
    "    filename = f\"{prefix}{folder_label}_{index:02d}.json\"\n",
    "    out_path = output_root / filename\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(pairs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def generate_category_files(folder_path, corpus_lines, output_path):\n",
    "    folder_label = folder_path.name\n",
    "    for prefix, full_label in LABEL.items():\n",
    "        labeled_path = folder_path / f\"{LABEL[prefix]}.json\"\n",
    "        labeled_pairs = load_labeled_pairs(labeled_path, prefix)\n",
    "\n",
    "        num_chunks = len(labeled_pairs) // 10\n",
    "        for i in range(num_chunks):\n",
    "            chunk_labeled = labeled_pairs[i * 10:(i + 1) * 10]\n",
    "            sampled_irrelevant = sample_irrelevant_pairs(corpus_lines, 990)\n",
    "            full = chunk_labeled + sampled_irrelevant\n",
    "            random.shuffle(full)\n",
    "            save_eval_file(full, output_path, prefix, folder_label, i)\n",
    "\n",
    "\n",
    "def generate_general_files(folder_path, corpus_lines, output_path):\n",
    "    folder_label = folder_path.name\n",
    "    all_labeled = {\n",
    "        key: load_labeled_pairs(folder_path / f\"{fname}.json\", key)\n",
    "        for key, fname in LABEL.items()\n",
    "    }\n",
    "\n",
    "    num_sets = min(\n",
    "        len(all_labeled['Qu']) // 1,\n",
    "        len(all_labeled['Fu']) // 3,\n",
    "        len(all_labeled['Pa']) // 2,\n",
    "        len(all_labeled['Si']) // 4\n",
    "    )\n",
    "\n",
    "    for i in range(num_sets):\n",
    "        block = []\n",
    "        for key, count in GENERAL_DISTRIBUTION.items():\n",
    "            start = i * count\n",
    "            block.extend(all_labeled[key][start:start + count])\n",
    "        sampled_irrelevant = sample_irrelevant_pairs(corpus_lines, 990)\n",
    "        full = block + sampled_irrelevant\n",
    "        random.shuffle(full)\n",
    "        save_eval_file(full, output_path, 'Ge', folder_label, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dc7049a-c5c2-4f86-9d40-f1833571739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lines = load_corpus_sentences(Path(\"data/corpus/corpus/corpus.jsonl\"))\n",
    "\n",
    "generate_category_files(Path(\"data/evaluation/eval-task-sources/S\"), corpus_lines, Path(\"data/evaluation/eval-tasks\"))\n",
    "generate_general_files(Path(\"data/evaluation/eval-task-sources/S\"), corpus_lines, Path(\"data/evaluation/eval-tasks\"))\n",
    "\n",
    "#generate_category_files(Path(\"data/evaluation/eval-task-sources/M\"), corpus_lines, Path(\"data/evaluation/eval-tasks\"))\n",
    "#generate_general_files(Path(\"data/evaluation/eval-task-sources/M\"), corpus_lines, Path(\"data/evaluation/eval-tasks\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc5c2e-b432-4ad2-bd0a-80dbcba5ba87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
