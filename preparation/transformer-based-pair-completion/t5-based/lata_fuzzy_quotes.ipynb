{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092080dc-5ae2-4daf-aa05-5b5776afde53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import spacy \n",
    "\n",
    "nlp = spacy.load('la_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dbbe80e-e8b5-46de-ad39-ff2b281a4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "LaTa = \"../../../models/LaTa\"\n",
    "PhilTa = \"../../../models/PhilTa\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PhilTa)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(PhilTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3922e57-a4d9-484d-949d-6e490a77041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 1: audiant illi, qui\n",
      "Option 2: audiant, qui\n",
      "Option 3: audiant:\n",
      "Option 4: ecclesiarum:\n",
      "Option 5: audiant, qui maxime\n"
     ]
    }
   ],
   "source": [
    "text = (\"Fill: Ecce audiant hoc illi, qui maxime ecclesiarum localium, id <extra_id_0> optime.\")\n",
    "\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=5,\n",
    "    num_beams=5,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5  \n",
    ")\n",
    "\n",
    "for i, output in enumerate(outputs, 1):\n",
    "    print(f\"Option {i}: {tokenizer.decode(output, skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ced347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 1: * *:om:.\n",
      "Option 2: * *:om.\n",
      "Option 3: * *:om:u:.\n",
      "Option 4: * *:om: in.\n",
      "Option 5: * *:om:u:a:.\n"
     ]
    }
   ],
   "source": [
    "text = (\"Fill: Ecce <extra_id_0> hoc illi, qui maxime ecclesiarum localium, id est coenobiorum, archimandritis detrahunt, quoties gregis sui patiuntur detrimentum, et cum ipsi uacent otio, temere operarios Dei diiudicant, ubi aliquos ex eis, qui hortatu ipsorum conuersi sunt ad saeculum relabi conspiciunt.\")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    num_beams=5,\n",
    "    do_sample=True,\n",
    "    top_k=100,\n",
    "    top_p=0.75,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5\n",
    ")\n",
    "\n",
    "for i, output in enumerate(outputs, 1):\n",
    "    print(f\"Option {i}: {tokenizer.decode(output, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demask(sentence: str) -> str:\n",
    "    \n",
    "    # Replace each \"<mask>\" with the appropriate T5 placeholder token\n",
    "    mask_index = 0\n",
    "    while \"<mask>\" in sentence:\n",
    "        sentence = sentence.replace(\"<mask>\", f\"<extra_id_{mask_index}>\", 1)\n",
    "        mask_index += 1\n",
    "\n",
    "    # Tokenize the input sentence and generate the demasked output\n",
    "    inputs = tokenizer(f\"Fill {sentence}\", return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7617d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(demask(\"Undecim discipuli <mask> in scholam ierunt.\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab7128e-a727-402b-831a-fdb5e581eb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "substructures = choose_substructures(\"Ecce audiant hoc illi, qui maxime ecclesiarum localium, id est coenobiorum, archimandritis detrahunt, quoties gregis sui patiuntur detrimentum, et cum ipsi uacent otio, temere operarios Dei diiudicant, ubi aliquos ex eis, qui hortatu ipsorum conuersi sunt ad saeculum relabi conspiciunt.\")\n",
    "for struct in substructures:\n",
    "    print(struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5297e28a-8879-4bfe-aba3-40b2fd80d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_substructures(sentence: str) -> List[str]:\n",
    "\n",
    "    len_sentence = len(sentence.split(' '))\n",
    "    min_len_subtree = (\n",
    "        math.ceil(len_sentence / 3) if len_sentence < 25 \n",
    "        else math.ceil(len_sentence / 7) if len_sentence < 50\n",
    "        else math.ceil(len_sentence / 15)\n",
    "    )\n",
    "\n",
    "    substructures = extract_substructures(sentence, min_len_subtree)\n",
    "    chosen_substructures = []\n",
    "    num_subtrees = math.ceil(len(substructures) / 3)\n",
    "    selection = random.sample(substructures, num_subtrees)\n",
    "\n",
    "    # Remove subsets from the sampled selection\n",
    "    selection.sort(key=len, reverse=True)  # Sort longest first\n",
    "    unique_selection = []\n",
    "\n",
    "    for sub in selection:\n",
    "        sub_set = set(sub.split())  # Convert substructure to a set of words\n",
    "        if not any(sub_set.issubset(set(added.split())) for added in unique_selection):\n",
    "            unique_selection.append(sub)\n",
    "\n",
    "    return unique_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c94102-3b9a-45b5-9cdf-d5b4ba845073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_substructures(sentence: str, min_len_subtree: int) -> List[str]:\n",
    "    doc = nlp(sentence)\n",
    "    substructures = set()\n",
    "    len_sentence = len(sentence.split(' '))\n",
    "    for token in doc:\n",
    "        subtree = \" \".join([t.text for t in token.subtree])\n",
    "        subtree = subtree.replace(',', '').strip()\n",
    "        subtree = subtree.replace('  ', ' ')\n",
    "        len_subtree = len(subtree.split(' '))\n",
    "        if(len_subtree > min_len_subtree and len_subtree < len_sentence):\n",
    "            substructures.add(subtree)\n",
    "    return list(substructures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42b264-1bf3-43de-a745-5355de57bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(file_path: str) -> List[str]:\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['sentence'].str.strip().str.strip('\"').tolist()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
