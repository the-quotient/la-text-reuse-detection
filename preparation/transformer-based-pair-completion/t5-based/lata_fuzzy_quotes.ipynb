{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092080dc-5ae2-4daf-aa05-5b5776afde53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import spacy \n",
    "\n",
    "nlp = spacy.load('la_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbbe80e-e8b5-46de-ad39-ff2b281a4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "LaTa = \"../../../models/LaTa\"\n",
    "PhilTa = \"../../../models/PhilTa\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PhilTa)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(PhilTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3922e57-a4d9-484d-949d-6e490a77041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"Fill: Ecce audiant hoc illi, qui maxime ecclesiarum localium, id <extra_id_0> optime.\")\n",
    "\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=5,\n",
    "    num_beams=5,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5  \n",
    ")\n",
    "\n",
    "for i, output in enumerate(outputs, 1):\n",
    "    print(f\"Option {i}: {tokenizer.decode(output, skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ced347",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"Fill: Ecce <extra_id_0> hoc illi, qui maxime ecclesiarum localium, id est coenobiorum, archimandritis detrahunt, quoties gregis sui patiuntur detrimentum, et cum ipsi uacent otio, temere operarios Dei diiudicant, ubi aliquos ex eis, qui hortatu ipsorum conuersi sunt ad saeculum relabi conspiciunt.\")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    num_beams=5,\n",
    "    do_sample=True,\n",
    "    top_k=100,\n",
    "    top_p=0.75,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5\n",
    ")\n",
    "\n",
    "for i, output in enumerate(outputs, 1):\n",
    "    print(f\"Option {i}: {tokenizer.decode(output, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demask(sentence: str) -> str:\n",
    "    \n",
    "    # Replace each \"<mask>\" with the appropriate T5 placeholder token\n",
    "    mask_index = 0\n",
    "    while \"<mask>\" in sentence:\n",
    "        sentence = sentence.replace(\"<mask>\", f\"<extra_id_{mask_index}>\", 1)\n",
    "        mask_index += 1\n",
    "\n",
    "    # Tokenize the input sentence and generate the demasked output\n",
    "    inputs = tokenizer(f\"Fill {sentence}\", return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7617d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(demask(\"Undecim discipuli <mask> in scholam ierunt.\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab7128e-a727-402b-831a-fdb5e581eb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "substructures = choose_substructures(\"Ecce audiant hoc illi, qui maxime ecclesiarum localium, id est coenobiorum, archimandritis detrahunt, quoties gregis sui patiuntur detrimentum, et cum ipsi uacent otio, temere operarios Dei diiudicant, ubi aliquos ex eis, qui hortatu ipsorum conuersi sunt ad saeculum relabi conspiciunt.\")\n",
    "for struct in substructures:\n",
    "    print(struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012f593-d774-4da1-90c4-cf6d826c4c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_substructures(sentence: str) -> List[str]:\n",
    "\n",
    "    len_sentence = len(sentence.split(' '))\n",
    "    min_len_subtree = (\n",
    "        math.ceil(len_sentence / 3) if len_sentence < 25 \n",
    "        else math.ceil(len_sentence / 7) if len_sentence < 50\n",
    "        else math.ceil(len_sentence / 15)\n",
    "    )\n",
    "\n",
    "    substructures = extract_substructures(sentence, min_len_subtree)\n",
    "    chosen_substructures = []\n",
    "    num_subtrees = math.ceil(len(substructures) / 3)\n",
    "    selection = random.sample(substructures, num_subtrees)\n",
    "\n",
    "    # Remove subsets from the sampled selection\n",
    "    selection.sort(key=len, reverse=True)  # Sort longest first\n",
    "    unique_selection = []\n",
    "\n",
    "    for sub in selection:\n",
    "        sub_set = set(sub.split())  # Convert substructure to a set of words\n",
    "        if not any(sub_set.issubset(set(added.split())) for added in unique_selection):\n",
    "            unique_selection.append(sub)\n",
    "\n",
    "    return unique_selection\n",
    "\n",
    "def extract_substructures(sentence: str, min_len_subtree: int) -> List[str]:\n",
    "    doc = nlp(sentence)\n",
    "    substructures = set()\n",
    "    len_sentence = len(sentence.split(' '))\n",
    "    for token in doc:\n",
    "        subtree = \" \".join([t.text for t in token.subtree])\n",
    "        subtree = subtree.replace(',', '').strip()\n",
    "        subtree = subtree.replace('  ', ' ')\n",
    "        len_subtree = len(subtree.split(' '))\n",
    "        if(len_subtree > min_len_subtree and len_subtree < len_sentence):\n",
    "            substructures.add(subtree)\n",
    "    return list(substructures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42b264-1bf3-43de-a745-5355de57bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(file_path: str) -> List[str]:\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['sentence'].str.strip().str.strip('\"').tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab6fd2-43a6-414e-a38f-ea59869ff97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from typing import List, Tuple, NamedTuple\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model (adjust as needed)\n",
    "nlp = spacy.load(\"la_core_web_lg\")\n",
    "\n",
    "class Substructure(NamedTuple):\n",
    "    text: str\n",
    "    start: int  # start index in the tokenized sentence\n",
    "    end: int    # end index (inclusive)\n",
    "    token_count: int\n",
    "\n",
    "def find_sublist(lst: List[str], sublst: List[str]) -> int:\n",
    "    \"\"\"Return the starting index of sublst in lst, or None if not found.\"\"\"\n",
    "    for i in range(len(lst) - len(sublst) + 1):\n",
    "        if lst[i : i + len(sublst)] == sublst:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def extract_substructures(sentence: str, tokens: List[str],\n",
    "                          region: str = \"middle\") -> List[Substructure]:\n",
    "    \"\"\"\n",
    "    Parse the sentence with spaCy and extract subtrees with fixed \n",
    "    minimal token counts based on the region.\n",
    "    \"\"\"\n",
    "\n",
    "    min_len = 1\n",
    "\n",
    "    total_tokens = len(tokens)\n",
    "    doc = nlp(sentence)\n",
    "    substructures = {}\n",
    "\n",
    "    for token in doc:\n",
    "        subtree_tokens = list(token.subtree)\n",
    "        subtree_text = \" \".join(t.text for t in subtree_tokens)\n",
    "        subtree_text = subtree_text.replace(\",\", \"\").strip()\n",
    "        subtree_text = \" \".join(subtree_text.split())\n",
    "\n",
    "        subtree_words = subtree_text.split()\n",
    "        len_subtree = len(subtree_words)\n",
    "\n",
    "        # Accept only if the subtree is long enough and not the full sentence.\n",
    "        if len_subtree > min_len and len_subtree < total_tokens:\n",
    "            start_idx = find_sublist(tokens, subtree_words)\n",
    "            if start_idx is not None:\n",
    "                end_idx = start_idx + len_subtree - 1\n",
    "                key = (start_idx, end_idx)\n",
    "                if key not in substructures:\n",
    "                    substructures[key] = Substructure(\n",
    "                        text=subtree_text, start=start_idx,\n",
    "                        end=end_idx, token_count=len_subtree\n",
    "                    )\n",
    "    return list(substructures.values())\n",
    "\n",
    "def choose_substructures_from_candidates(\n",
    "    candidates: List[Substructure], target_mask: int\n",
    ") -> List[Substructure]:\n",
    "    \"\"\"\n",
    "    Greedily choose substructures from the candidate list whose total token \n",
    "    count does not exceed the target_mask.\n",
    "    \"\"\"\n",
    "    candidates.sort(key=lambda s: s.token_count, reverse=True)\n",
    "    selected = []\n",
    "    current_mask = 0\n",
    "    for sub in candidates:\n",
    "        if current_mask + sub.token_count <= target_mask:\n",
    "            selected.append(sub)\n",
    "            current_mask += sub.token_count\n",
    "    return selected\n",
    "\n",
    "def apply_masking(tokens: List[str], selected_subs: List[Substructure]\n",
    "                 ) -> List[str]:\n",
    "    \"\"\"\n",
    "    Replace each selected substructure with a single \"[MASK]\" token.\n",
    "    This function assumes that the substructures do not overlap.\n",
    "    \"\"\"\n",
    "    # Sort the selected substructures by their start index\n",
    "    selected_subs = sorted(selected_subs, key=lambda s: s.start)\n",
    "    masked_tokens = []\n",
    "    i = 0\n",
    "    for sub in selected_subs:\n",
    "        # Append tokens before the substructure\n",
    "        while i < sub.start:\n",
    "            masked_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "        # Insert a single mask token for the entire substructure\n",
    "        masked_tokens.append(\"[MASK]\")\n",
    "        # Skip over tokens that are part of the masked substructure\n",
    "        i = sub.end + 1\n",
    "    # Append any tokens remaining after the last substructure\n",
    "    while i < len(tokens):\n",
    "        masked_tokens.append(tokens[i])\n",
    "        i += 1\n",
    "    return masked_tokens\n",
    "\n",
    "def mask_sentence(\n",
    "    sentence: str, region: str = \"middle\",\n",
    "    mask_ratio: Tuple[float, float] = (0.1, 0.2)\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Mask 10% to 20% of the sentenceâ€™s tokens by replacing entire substructures \n",
    "    (as extracted by extract_substructures) with a mask token.\n",
    "    The 'region' parameter toggles between 'start', 'middle', and 'end' \n",
    "    of the sentence.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.text for token in doc]\n",
    "    total_tokens = len(tokens)\n",
    "    \n",
    "    # Determine a random target mask count within the given ratio.\n",
    "    min_mask = math.ceil(total_tokens * mask_ratio[0])\n",
    "    max_mask = math.floor(total_tokens * mask_ratio[1])\n",
    "    target_mask = random.randint(min_mask, max_mask)\n",
    "    \n",
    "    # Extract candidate substructures (with fixed minimal length).\n",
    "    candidates = extract_substructures(sentence, tokens, region)\n",
    "    \n",
    "    # If no candidates remain in the desired region, fallback to using all.\n",
    "    if not candidates:\n",
    "        candidates = extract_substructures(sentence, tokens)\n",
    "    \n",
    "    selected_subs = choose_substructures_from_candidates(candidates, target_mask)\n",
    "    \n",
    "    masked_tokens = apply_masking(tokens, selected_subs)\n",
    "    \n",
    "    return \" \".join(masked_tokens)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    test_sentence = (\n",
    "        \"Ecce audiant hoc illi, qui maxime ecclesiarum localium, id est coenobiorum, \"\n",
    "        \"archimandritis detrahunt, quoties gregis sui patiuntur detrimentum, et cum ipsi \"\n",
    "        \"uacent otio, temere operarios Dei diiudicant, ubi aliquos ex eis, qui hortatu ipsorum \"\n",
    "        \"conuersi sunt ad saeculum relabi conspiciunt.\"\n",
    "    )\n",
    "    for region in [\"start\", \"middle\", \"end\"]:\n",
    "        print(mask_sentence(test_sentence, region=region))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab208b-4419-473f-908a-f6085d660539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
