{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a716c5bd-8466-4a45-947e-f1c331a98eec",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2827d7-b50e-4a18-902d-c370c209623e",
   "metadata": {},
   "source": [
    "**Data Preprocessing Pipeline**\n",
    "\n",
    "- Split large files (memory constraints with spacy)\n",
    "- Sentence segmentation with LatinCy\n",
    "- Cleaning the sentences with regex\n",
    "- Filter short sentences and combine the files\n",
    "- Transform into .jsonl\n",
    "\n",
    "In order to save checkpoints and ensure reproducibility, after each step the folder has been duplicated and renamed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8436cf45-2775-4ed4-bce4-9ab02b946d7b",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dfd5ca-2f96-4c53-a056-8b6ad8aafff7",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d89d3-10be-4003-9967-6d193abaa816",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_large_files(\"../../data/preprocessing/1-smaller-files/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a8e95-779a-48f4-911c-0a58c5bbd465",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_segmentation(\"../../data/preprocessing/2-segmented-sentences/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487ef63-8c6b-4838-96e9-ea26b4233164",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences(\"../../data/preprocessing/3-cleaned-sentences/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478daec-60c4-493f-9933-a07a8b0fdb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_short_sentences(\"../../data/preprocessing/4-only-longer-sentences/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae0be6-0e16-4f39-925b-9ac018779a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_to_jsonl(\"../../data/preprocessing/4-only-longer-sentences/\", \"../../data/corpus/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28626bb5-47bd-4c9e-88b9-f5de70a50087",
   "metadata": {},
   "source": [
    "## Imports and Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2287161-3b8b-4265-9e85-db5bef5f75bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import re \n",
    "import json\n",
    "\n",
    "nlp = spacy.load(\"la_core_web_lg\")\n",
    "nlp.max_length = 4000000\n",
    "\n",
    "TOKEN_LIMIT = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b08e9e-650c-4557-bb33-6a9322b24c9f",
   "metadata": {},
   "source": [
    "## Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38cdd5-67f1-496d-8f95-d71acd84423c",
   "metadata": {},
   "source": [
    "### Splitting Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845f4b2-49f8-42be-be9d-1f2b49ae9524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_large_files(root_folder, max_lines=250):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_folder):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                    line_count = len(lines)\n",
    "                    \n",
    "                    if line_count > max_lines:\n",
    "                        base_name, ext = os.path.splitext(filename)\n",
    "                        \n",
    "                        for i in range(0, line_count, max_lines):\n",
    "                            part_lines = lines[i:i + max_lines]\n",
    "                            part_filename = f\"{base_name}_part{i // max_lines + 1}{ext}\"\n",
    "                            part_path = os.path.join(dirpath, part_filename)\n",
    "                            \n",
    "                            with open(part_path, 'w', encoding='utf-8') as part_file:\n",
    "                                part_file.writelines(part_lines)\n",
    "                                \n",
    "                        os.remove(file_path)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397cb686-8893-41b8-a145-93304806698d",
   "metadata": {},
   "source": [
    "### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a4e7f-05a8-4c63-a75f-d4cfd51ad5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_segmentation(root_folder):\n",
    "    for subdir, _, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.txt'):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                sentences = sentence_segmentation_in_text(content)\n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    for sentence in sentences:\n",
    "                        f.write(sentence + \"\\n\")\n",
    "                    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab153728-aafe-42fb-8605-49dc5c95f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_segmentation_in_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e80c52-bbb3-4012-a7f6-9a4646f20216",
   "metadata": {},
   "source": [
    "### Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd886c5-3714-472a-8049-563bb82b918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(folder):\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    sentences = f.readlines()\n",
    "                \n",
    "                # Process each sentence: strip whitespace, then apply both cleaning functions\n",
    "                cleaned_sentences = []\n",
    "                for sentence in sentences:\n",
    "                    cleaned_sentence = clean(sentence)\n",
    "                    cleaned_sentence = normalize_capitalization(cleaned_sentence)\n",
    "                    cleaned_sentences.append(cleaned_sentence)\n",
    "                \n",
    "                # Write the cleaned sentences back to the file\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(\"\\n\".join(cleaned_sentences))\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e9644-467d-4403-8c99-2c91450e4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    patterns = [\n",
    "        r'\\bpage\\b\\s*\\d*',                    # 'page' followed by numbers\n",
    "        r'\\b\\w*\\d+\\w*\\b',                     # alphanumeric with digits\n",
    "        r'\\[.*?\\]',                           # brackets and content\n",
    "        r'\\bUERS\\b[.,\\s]*',                   # 'UERS' case-insensitive\n",
    "        r'\\bCAPUT\\b\\s*[IVXLCDM]+\\.',          # 'CAPUT' + Roman numerals\n",
    "        r'\\bCAP\\.\\s*[IVXLCDM]+\\.',            # 'CAP.' + Roman numerals\n",
    "        r'\\bGo back to text\\b',               # specific phrase\n",
    "        r'\\bFront Matter\\b',                  # specific phrase\n",
    "        r'^(\\b\\w+\\b[.,\\s]*){1,3}$',           # short alphanumeric sequences\n",
    "        r'\\.{2,}',                            # multiple periods\n",
    "        r'([.,\\s])\\1{1,}',                    # repeated punctuation/spaces\n",
    "        r'[,.]{2,}',                          # mixed punctuations\n",
    "        r'^\\s*[.,]+',                         # leading punctuation/spaces\n",
    "        r'^\\b[IVXLCDM]+\\b\\.?',                # leading Roman numerals\n",
    "        r'^\\bibid\\b\\.?',                      # leading 'ibid.' case-insensitive\n",
    "        r'\\b[a-z]\\.\\b',                       # Removes standalone single letters (vowels + consonants) with a period\n",
    "        r'\\b[b-df-hj-np-tv-z]\\b'              # Removes standalone consonants without a period\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace non-word characters (except spaces, dots, and commas) with a space\n",
    "    text = re.sub(r'[^\\w\\s.,]', ' ', text)\n",
    "\n",
    "    # Remove spaces before commas and periods\n",
    "    text = re.sub(r'\\s+([,.])', r'\\1', text)\n",
    "\n",
    "    # Ensure space after punctuation (if followed by a letter or number)\n",
    "    text = re.sub(r'([,.])(\\w)', r'\\1 \\2', text)\n",
    "\n",
    "    # Normalize spaces (remove extra spaces)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf6789-4c94-42af-8070-aac9acebdc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_capitalization(text): \n",
    "    processed_tokens = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.isupper():  # Check if the token is all uppercase\n",
    "            if token.ent_type_:  # If it's a named entity, capitalize only the first letter\n",
    "                processed_tokens.append(token.text.capitalize() + token.whitespace_)\n",
    "            else:  # Otherwise, make it all lowercase\n",
    "                processed_tokens.append(token.text.lower() + token.whitespace_)\n",
    "        else:\n",
    "            processed_tokens.append(token.text + token.whitespace_)  # Preserve original spacing\n",
    "    \n",
    "    return \"\".join(processed_tokens)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b4f00-41d0-450e-8497-3bbffeb5cd75",
   "metadata": {},
   "source": [
    "### Filtering Short Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c56eca-5a4a-49cd-88f7-f254b902a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_sentences(folder):\n",
    "    filter_short_sentences_per_author(folder, TOKEN_LIMIT, \"Brenz\")\n",
    "    filter_short_sentences_per_author(folder, TOKEN_LIMIT, \"Bucer\")\n",
    "    filter_short_sentences_per_author(folder, TOKEN_LIMIT, \"Bugenhagen\")\n",
    "    filter_short_sentences_per_author(folder, TOKEN_LIMIT, \"Bullinger\")\n",
    "    filter_short_sentences_per_author(folder, TOKEN_LIMIT, \"Erasmus\")\n",
    "    filter_short_sentences_per_author(folder, TOKEN_LIMIT, \"Melanchthon\")\n",
    "    filter_short_sentences_per_author(folder, TOKEN_LIMIT, \"Oekolampad\")\n",
    "    filter_short_sentences_per_author(folder, TOKEN_LIMIT, \"Theophylact\")\n",
    "    filter_short_sentences_per_author(folder, TOKEN_LIMIT, \"Tuitiensis\")\n",
    "    filter_short_sentences_per_author(folder, TOKEN_LIMIT, \"Wild\")\n",
    "    filter_short_sentences_per_author(folder, TOKEN_LIMIT, \"Zwingli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b219d8-b958-4f40-b3cb-971fda5345cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_sentences_per_author(folder, token_limit, author):\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt') and file.startswith(author):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    # Each line is considered a sentence\n",
    "                    for line in f:\n",
    "                        sentence = line.strip()\n",
    "                        if sentence:  # Skip empty lines\n",
    "                            # Tokenize the sentence using spaCy\n",
    "                            doc = nlp(sentence)\n",
    "                            if len(doc) > token_limit:\n",
    "                                sentences.append(sentence)\n",
    "    \n",
    "    with open(os.path.join(folder, f\"{author}.txt\"), 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence.strip() + \"\\n\")\n",
    "        print(\"Finished \" + author) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93bb72e-cd46-4ed7-b43f-57f7edba414c",
   "metadata": {},
   "source": [
    "### Transforming into jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a77df5-a1b0-49ea-a3e5-e4ed60e19224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_jsonl(input_folder, output_folder):\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            txt_to_jsonl(file_path, output_folder)\n",
    "            print(\"Finished \" + file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb4b0b1-612c-4cf8-a470-80b4af45c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_jsonl(file_path, output_folder):\n",
    "\n",
    "    root, ext = os.path.splitext(file_path)\n",
    "    jsonl_file = os.path.join(output_folder, root) + \".jsonl\"\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as infile, open(jsonl_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            sentence = line.strip()\n",
    "            if sentence:  # Skip empty lines\n",
    "                json_obj = {\"sentence\": sentence}\n",
    "                outfile.write(json.dumps(json_obj, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff03e9-d7a9-4a58-8fdf-2336ee28c14c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
