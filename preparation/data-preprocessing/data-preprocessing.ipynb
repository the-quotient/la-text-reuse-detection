{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a716c5bd-8466-4a45-947e-f1c331a98eec",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2827d7-b50e-4a18-902d-c370c209623e",
   "metadata": {},
   "source": [
    "**Data Preprocessing Pipeline**\n",
    "\n",
    "- Split large files (memory constraints with spacy)\n",
    "- Sentence segmentation with LatinCy\n",
    "- Cleaning the sentences with regex\n",
    "- Filter short sentences\n",
    "- Reconstruct the document files\n",
    "- build author files\n",
    "- build one corpus file\n",
    "- Transform into .jsonl\n",
    "\n",
    "In order to save checkpoints and ensure reproducibility, after each step the folder has been duplicated and renamed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8436cf45-2775-4ed4-bce4-9ab02b946d7b",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dfd5ca-2f96-4c53-a056-8b6ad8aafff7",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d89d3-10be-4003-9967-6d193abaa816",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_large_files(\"../../data/preprocessing/1-smaller-files/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a8e95-779a-48f4-911c-0a58c5bbd465",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_segmentation(\"../../data/preprocessing/2-segmented-sentences/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487ef63-8c6b-4838-96e9-ea26b4233164",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences(\"../../data/preprocessing/3-cleaned-sentences/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478daec-60c4-493f-9933-a07a8b0fdb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_short_sentences(\"../../data/preprocessing/4-only-longer-sentences/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8fe59c-2bd8-463c-b5f1-b01837151f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct_documents(\"../../data/preprocessing/4-only-longer-sentences/\", \"../../data/preprocessing/5-documents/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775bb44-9868-45c4-8744-e5037bb6ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_author_files(\"../../data/preprocessing/5-documents/\", \"../../data/preprocessing/6-authors/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d174d1b-8567-4754-a744-a535c13b5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_corpus_file(\"../../data/preprocessing/6-authors/\", \"../../data/preprocessing/7-corpus/corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae0be6-0e16-4f39-925b-9ac018779a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_to_jsonl(\"../../data/preprocessing/5-documents/\", \"../../data/corpus/documents/\")\n",
    "transform_to_jsonl(\"../../data/preprocessing/6-authors/\", \"../../data/corpus/authors/\")\n",
    "transform_to_jsonl(\"../../data/preprocessing/7-corpus/\", \"../../data/corpus/corpus/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28626bb5-47bd-4c9e-88b9-f5de70a50087",
   "metadata": {},
   "source": [
    "## Imports and Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2287161-3b8b-4265-9e85-db5bef5f75bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import re \n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "nlp = spacy.load(\"la_core_web_lg\")\n",
    "nlp.max_length = 4000000\n",
    "\n",
    "TOKEN_LIMIT = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b08e9e-650c-4557-bb33-6a9322b24c9f",
   "metadata": {},
   "source": [
    "## Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38cdd5-67f1-496d-8f95-d71acd84423c",
   "metadata": {},
   "source": [
    "### Splitting Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845f4b2-49f8-42be-be9d-1f2b49ae9524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_large_files(root_folder, max_lines=250):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_folder):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                    line_count = len(lines)\n",
    "                    \n",
    "                    if line_count > max_lines:\n",
    "                        base_name, ext = os.path.splitext(filename)\n",
    "                        \n",
    "                        for i in range(0, line_count, max_lines):\n",
    "                            part_lines = lines[i:i + max_lines]\n",
    "                            part_filename = f\"{base_name}_part{i // max_lines + 1}{ext}\"\n",
    "                            part_path = os.path.join(dirpath, part_filename)\n",
    "                            \n",
    "                            with open(part_path, 'w', encoding='utf-8') as part_file:\n",
    "                                part_file.writelines(part_lines)\n",
    "                                \n",
    "                        os.remove(file_path)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397cb686-8893-41b8-a145-93304806698d",
   "metadata": {},
   "source": [
    "### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a4e7f-05a8-4c63-a75f-d4cfd51ad5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_segmentation(root_folder):\n",
    "    for subdir, _, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.txt'):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                sentences = sentence_segmentation_in_text(content)\n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    for sentence in sentences:\n",
    "                        f.write(sentence + \"\\n\")\n",
    "                    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab153728-aafe-42fb-8605-49dc5c95f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_segmentation_in_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e80c52-bbb3-4012-a7f6-9a4646f20216",
   "metadata": {},
   "source": [
    "### Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd886c5-3714-472a-8049-563bb82b918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(folder):\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    sentences = f.readlines()\n",
    "                \n",
    "                # Process each sentence: strip whitespace, then apply both cleaning functions\n",
    "                cleaned_sentences = []\n",
    "                for sentence in sentences:\n",
    "                    cleaned_sentence = clean(sentence)\n",
    "                    cleaned_sentence = normalize_capitalization(cleaned_sentence)\n",
    "                    cleaned_sentences.append(cleaned_sentence)\n",
    "                \n",
    "                # Write the cleaned sentences back to the file\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(\"\\n\".join(cleaned_sentences))\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e9644-467d-4403-8c99-2c91450e4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    patterns = [\n",
    "        r'\\bpage\\b\\s*\\d*',                    # 'page' followed by numbers\n",
    "        r'\\b\\w*\\d+\\w*\\b',                     # alphanumeric with digits\n",
    "        r'\\[.*?\\]',                           # brackets and content\n",
    "        r'\\bUERS\\b[.,\\s]*',                   # 'UERS' case-insensitive\n",
    "        r'\\bCAPUT\\b\\s*[IVXLCDM]+\\.',          # 'CAPUT' + Roman numerals\n",
    "        r'\\bCAP\\.\\s*[IVXLCDM]+\\.',            # 'CAP.' + Roman numerals\n",
    "        r'\\bGo back to text\\b',               # specific phrase\n",
    "        r'\\bFront Matter\\b',                  # specific phrase\n",
    "        r'^(\\b\\w+\\b[.,\\s]*){1,3}$',           # short alphanumeric sequences\n",
    "        r'\\.{2,}',                            # multiple periods\n",
    "        r'([.,\\s])\\1{1,}',                    # repeated punctuation/spaces\n",
    "        r'[,.]{2,}',                          # mixed punctuations\n",
    "        r'^\\s*[.,]+',                         # leading punctuation/spaces\n",
    "        r'^\\b[IVXLCDM]+\\b\\.?',                # leading Roman numerals\n",
    "        r'^\\bibid\\b\\.?',                      # leading 'ibid.' case-insensitive\n",
    "        r'\\b[a-z]\\.\\b',                       # Removes standalone single letters (vowels + consonants) with a period\n",
    "        r'\\b[b-df-hj-np-tv-z]\\b'              # Removes standalone consonants without a period\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace non-word characters (except spaces, dots, and commas) with a space\n",
    "    text = re.sub(r'[^\\w\\s.,]', ' ', text)\n",
    "\n",
    "    # Remove spaces before commas and periods\n",
    "    text = re.sub(r'\\s+([,.])', r'\\1', text)\n",
    "\n",
    "    # Ensure space after punctuation (if followed by a letter or number)\n",
    "    text = re.sub(r'([,.])(\\w)', r'\\1 \\2', text)\n",
    "\n",
    "    # Normalize spaces (remove extra spaces)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf6789-4c94-42af-8070-aac9acebdc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_capitalization(text): \n",
    "    processed_tokens = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text.isupper():  # Check if the token is all uppercase\n",
    "            if token.ent_type_:  # If it's a named entity, capitalize only the first letter\n",
    "                processed_tokens.append(token.text.capitalize() + token.whitespace_)\n",
    "            else:  # Otherwise, make it all lowercase\n",
    "                processed_tokens.append(token.text.lower() + token.whitespace_)\n",
    "        else:\n",
    "            processed_tokens.append(token.text + token.whitespace_)  # Preserve original spacing\n",
    "    \n",
    "    return \"\".join(processed_tokens)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b4f00-41d0-450e-8497-3bbffeb5cd75",
   "metadata": {},
   "source": [
    "### Filtering Short Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f667b07-3dcc-4848-8b2c-d5d4f3c24d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_sentences(folder, token_limit=TOKEN_LIMIT):\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                sentences = []\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    lines = [line.strip() for line in f if line.strip()]\n",
    "                \n",
    "                # Tokenize using spaCy's efficient pipeline\n",
    "                docs = list(nlp.pipe(lines))\n",
    "                \n",
    "                # Filter sentences based on token length\n",
    "                sentences = [sent.text for sent in docs if len(sent) > token_limit]\n",
    "                \n",
    "                # Overwrite the original file\n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(\"\\n\".join(sentences) + \"\\n\")\n",
    "                \n",
    "                print(f\"Finished processing {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c199348-8fc1-4964-8dad-fbdd9c7f819c",
   "metadata": {},
   "source": [
    "### Reconstruct Document Files and build Author files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed800d99-6dc6-4cef-a933-8b0c492621f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_documents(input_folder, output_folder):\n",
    "    # Regex pattern to extract the \"Author_Document_\" prefix\n",
    "    pattern = re.compile(r\"^([A-Za-z0-9_-]+_[A-Za-z0-9_-]+)_\")\n",
    "    \n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for root, _, files in os.walk(input_folder):\n",
    "        merge(pattern, root, output_folder, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6802eb41-81e7-47f5-af4f-2cb1d266876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_author_files(input_folder, output_folder):\n",
    "    # Regex pattern to extract the \"Author_\" prefix\n",
    "    pattern = re.compile(r\"^([A-Za-z0-9_-]+)_\")\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    merge(pattern, input_folder, output_folder, os.listdir(input_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2223cc4-0b05-4f10-82f9-ae8b960e9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(pattern, input_folder, output_folder, files):\n",
    "    # Dictionary to store file paths grouped by prefix\n",
    "    file_groups = defaultdict(list)\n",
    "\n",
    "    # Scan the directory and group files by prefix\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            match = pattern.match(filename)\n",
    "            if match:\n",
    "                prefix = match.group(1)\n",
    "                file_groups[prefix].append(os.path.join(input_folder, filename))\n",
    "\n",
    "    # Merge files for each prefix\n",
    "    for prefix, file_list in file_groups.items():\n",
    "        merged_filepath = os.path.join(output_folder, f\"{prefix}.txt\")\n",
    "\n",
    "        with open(merged_filepath, \"w\", encoding=\"utf-8\") as merged_file:\n",
    "            for file_path in file_list:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "                    merged_file.write(infile.read())  # Add spacing between files\n",
    "\n",
    "        print(f\"Merged {len(file_list)} files into {merged_filepath}\")\n",
    "\n",
    "    print(\"Merging complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8c0579-f58b-49fd-9155-67e5b9af8b56",
   "metadata": {},
   "source": [
    "### Build Corpus File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f14a5-afc2-4a25-86f4-7d46f9b448aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus_file(input_folder, output_file):\n",
    "\n",
    "    # Get all txt files in the directory\n",
    "    txt_files = [f for f in os.listdir(input_folder) if f.endswith(\".txt\")]\n",
    "\n",
    "    # Merge all txt files into one\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as merged_file:\n",
    "        for txt_file in txt_files:\n",
    "            file_path = os.path.join(input_folder, txt_file)\n",
    "        \n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "                merged_file.write(infile.read()+ \"\\n\")\n",
    "        \n",
    "            print(f\"Merged: {txt_file}\")\n",
    "\n",
    "    print(f\"\\nAll files merged into: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93bb72e-cd46-4ed7-b43f-57f7edba414c",
   "metadata": {},
   "source": [
    "### Transforming into jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf7bc5-cf69-4aac-8f5c-385decdd22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_jsonl(input_folder, output_folder):\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for file in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        if os.path.isfile(file_path):  # Process only files\n",
    "            txt_to_jsonl(file_path, output_folder)\n",
    "            print(f\"Finished processing {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853399b-037c-440c-8370-2f7716cedb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_jsonl(file_path, output_folder):\n",
    "    filename, ext = os.path.splitext(os.path.basename(file_path))  # Extract filename only\n",
    "    jsonl_file = os.path.join(output_folder, filename + \".jsonl\")\n",
    "\n",
    "    # Ensure the JSONL file exists (though open() will create it if it doesn't)\n",
    "    with open(file_path, 'r', encoding='utf-8') as infile, open(jsonl_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            sentence = line.strip()\n",
    "            if sentence:  # Skip empty lines\n",
    "                json_obj = {\"sentence\": sentence}\n",
    "                outfile.write(json.dumps(json_obj, ensure_ascii=False) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
